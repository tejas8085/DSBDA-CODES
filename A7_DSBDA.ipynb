{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Text Analytics**\n","1. Extract Sample document and apply following document preprocessing methods:\n","Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n","2. Create representation of document by calculating Term Frequency and Inverse Document \n","Frequency"],"metadata":{"id":"XWLIqDqb8vdt"}},{"cell_type":"markdown","source":["Import Required Libraries"],"metadata":{"id":"V35X5Mu-9saJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hoUzpEo5oiLf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"201abf13-4c7c-41a3-d709-45453919650e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n"]}],"source":["pip install nltk"]},{"cell_type":"code","source":["import nltk\n","import re"],"metadata":{"id":"FDAhtmS7xcFt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" Download the required packages"],"metadata":{"id":"wT_jSv0Z9yW4"}},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8PY6YJ-xz4i","outputId":"bf89a8b5-9d75-405d-f8fb-30ab2608637c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Initialize the text"],"metadata":{"id":"BCf1JSqr92s3"}},{"cell_type":"code","source":["text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\""],"metadata":{"id":"CTZnoVCqyJfy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perform Tokenization"],"metadata":{"id":"wH0sUOt097eX"}},{"cell_type":"code","source":["#Sentence Tokenization\n","from nltk.tokenize import sent_tokenize\n","tokenized_text= sent_tokenize(text)\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e1kIOrvPyRUq","outputId":"347ab39c-54a0-4e9d-cfdb-7fe389a7f10c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"]}]},{"cell_type":"code","source":["#Word Tokenization\n","from nltk.tokenize import word_tokenize\n","tokenized_word=word_tokenize(text)\n","print(tokenized_word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koVGC1I8ybka","outputId":"daf803b3-15db-4d2e-87cd-7eb81e031a53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"]}]},{"cell_type":"markdown","source":["Removing Punctuations and Stop Words"],"metadata":{"id":"9HeGl9rF9__j"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop_words=set(stopwords.words(\"english\"))\n","print(stop_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynzLQb9dzAOS","outputId":"a8b6dfcf-d293-453f-d168-45d9a1a19afd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\"that'll\", 'doing', 'so', 'once', 'most', 'having', 'up', 'and', 'on', 'will', 'same', 'some', 'ma', 'an', \"you'd\", 'each', 'in', 'll', 'is', 'between', 'herself', \"mightn't\", 'before', 'have', \"you'll\", 'above', 'mustn', 'where', 'she', 'these', 'more', 'hers', 'then', 'now', 'after', 'his', 'by', 'whom', 'yours', 't', 'wouldn', 'off', 'when', 'be', 'shouldn', 'themselves', 'had', 'until', 'other', 'how', 'few', 'myself', 'should', 'himself', 'below', \"hasn't\", 'does', 'ain', 'has', 'am', \"you've\", 'over', 'it', 'aren', \"needn't\", 'down', 'only', 'were', 'are', 'don', 'couldn', 'him', \"you're\", 've', 'doesn', 'theirs', \"she's\", 'this', 'they', 'again', \"shouldn't\", 'that', 'about', 'as', \"mustn't\", 'their', 'my', 'ourselves', \"couldn't\", 'to', 'didn', 'its', 'there', 'do', 'why', 'not', \"doesn't\", 'was', 'needn', 'while', 'me', 'can', 'during', 'very', \"didn't\", 'a', 'isn', 'the', \"aren't\", 'i', 'if', 'through', \"shan't\", 'just', 'who', 'won', 'ours', 'what', 'here', 'm', 'no', 're', 'hasn', 'wasn', 'but', \"should've\", 'against', \"won't\", 'which', 'shan', 'her', 'he', 'too', 'd', 'been', 'weren', 'with', 'under', 'any', \"isn't\", 'such', 'nor', 'your', 'because', 'them', 'we', 'haven', \"weren't\", \"haven't\", 'hadn', 'at', \"wasn't\", 'from', 'than', 'o', 'did', 'all', 'own', \"wouldn't\", 'those', 'both', 'for', 'yourself', 'itself', 'being', 'y', 'yourselves', 's', 'you', 'our', 'into', 'out', 'of', 'mightn', \"hadn't\", \"don't\", \"it's\", 'further', 'or'}\n"]}]},{"cell_type":"code","source":["text= \"How to remove stop words with NLTK library in Python?\"\n","text= re.sub('[^a-zA-Z]', ' ',text)\n","tokens = word_tokenize(text.lower())\n","filtered_text=[]\n","for w in tokens:\n","  if w not in stop_words:\n","    filtered_text.append(w)\n","print(\"Tokenized Sentence:\",tokens)\n","print(\"Filterd Sentence:\",filtered_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGr11viVzJ0y","outputId":"ac121e18-1454-43d7-be0a-4ef42a00ccef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n","Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"]}]},{"cell_type":"markdown","source":["Perform Stemming"],"metadata":{"id":"l4B_tVBT-Gtd"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n","ps =PorterStemmer()\n","for w in e_words:\n","  rootWord=ps.stem(w)\n","print(rootWord)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hr44_RIWzrAC","outputId":"19c75eaa-889a-431b-9603-155e1b108e86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wait\n"]}]},{"cell_type":"markdown","source":["Perform Lemmatization"],"metadata":{"id":"YGBMXou--Kzn"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","text = \"studies studying cries cry\"\n","tokenization = nltk.word_tokenize(text)\n","for w in tokenization:\n","  print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCLDETze0GGi","outputId":"793b1a2c-f54e-41bd-bbaa-e4149886862b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemma for studies is study\n","Lemma for studying is studying\n","Lemma for cries is cry\n","Lemma for cry is cry\n"]}]},{"cell_type":"markdown","source":["Apply POS Tagging to text"],"metadata":{"id":"flbfJ9Sr-Ovg"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","data=\"The pink sweater fit her perfectly\"\n","words=word_tokenize(data)\n","for word in words:\n","  print(nltk.pos_tag([word]))"],"metadata":{"id":"55KAG-U10MNh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"53a28d4e-b8ad-4614-a6c9-da63e9d0cb7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('The', 'DT')]\n","[('pink', 'NN')]\n","[('sweater', 'NN')]\n","[('fit', 'NN')]\n","[('her', 'PRP$')]\n","[('perfectly', 'RB')]\n"]}]},{"cell_type":"markdown","source":["**Part II**\n","\n","Import the necessary libraries"],"metadata":{"id":"X6dufOpt-SAA"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"id":"5M9ZRzcyGyAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initialize the Documents"],"metadata":{"id":"F3hT-fxw-ahF"}},{"cell_type":"code","source":["documentA = 'Jupiter is the largest planet'\n","documentB = 'Mars is the fourth planet from the Sun'"],"metadata":{"id":"f3TA4LfIG34A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create BagofWords (BoW) for Document A and B"],"metadata":{"id":"R6DDQR-1-dWl"}},{"cell_type":"code","source":["bagOfWordsA = documentA.split(' ')\n","bagOfWordsB = documentB.split(' ')"],"metadata":{"id":"yRt5QK3rG7Eo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create Collection of Unique words from Document A and B"],"metadata":{"id":"34rp4Kt9-h_B"}},{"cell_type":"code","source":["uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"],"metadata":{"id":"U8ptp9BrG_4Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a dictionary of words and their occurrence for each document in the corpus"],"metadata":{"id":"kizZAX9C-lUs"}},{"cell_type":"code","source":["numOfWordsA = dict.fromkeys(uniqueWords, 0)\n","for word in bagOfWordsA:\n","  numOfWordsA[word] += 1\n","  numOfWordsB = dict.fromkeys(uniqueWords, 0)\n","\n","for word in bagOfWordsB:\n","  numOfWordsB[word] += 1  "],"metadata":{"id":"R1w07Bx0HDWA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute the term frequency for each of our documents"],"metadata":{"id":"IVSPbCAS-otR"}},{"cell_type":"code","source":["def computeTF(wordDict, bagOfWords):\n","  tfDict = {}\n","  bagOfWordsCount = len(bagOfWords)\n","  for word, count in wordDict.items():\n","    tfDict[word] = count / float(bagOfWordsCount)\n","  return tfDict\n","tfA = computeTF(numOfWordsA, bagOfWordsA)\n","tfB = computeTF(numOfWordsB, bagOfWordsB)"],"metadata":{"id":"yRCOrIzMHL9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tfA"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qt_LgHkMXGKP","outputId":"74b0cafd-06e6-4dbb-ce75-5ed474adf1a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'planet': 0.2,\n"," 'Jupiter': 0.2,\n"," 'is': 0.2,\n"," 'from': 0.0,\n"," 'Mars': 0.0,\n"," 'Sun': 0.0,\n"," 'the': 0.2,\n"," 'fourth': 0.0,\n"," 'largest': 0.2}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["tfB"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVuf4Tm_XLAD","outputId":"bfaff691-055f-4776-b96b-7cb86b476d0b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'planet': 0.125,\n"," 'Jupiter': 0.0,\n"," 'is': 0.125,\n"," 'from': 0.125,\n"," 'Mars': 0.125,\n"," 'Sun': 0.125,\n"," 'the': 0.25,\n"," 'fourth': 0.125,\n"," 'largest': 0.0}"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["Compute the term Inverse Document Frequency"],"metadata":{"id":"l9enRDwm-xca"}},{"cell_type":"code","source":["def computeIDF(documents):\n","  import math\n","  N = len(documents)\n","  idfDict = dict.fromkeys(documents[0].keys(), 0)\n","  for document in documents:\n","    for word, val in document.items():\n","      if val > 0:\n","        idfDict[word] += 1\n","  for word, val in idfDict.items():\n","      idfDict[word] = math.log(N / float(val))\n","  return idfDict\n","idfs = computeIDF([numOfWordsA, numOfWordsB])\n","idfs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAML0rqfPSQA","outputId":"f83f0064-d163-4107-9a76-e291e6da25f1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'planet': 0.0,\n"," 'Jupiter': 0.6931471805599453,\n"," 'is': 0.0,\n"," 'from': 0.6931471805599453,\n"," 'Mars': 0.6931471805599453,\n"," 'Sun': 0.6931471805599453,\n"," 'the': 0.0,\n"," 'fourth': 0.6931471805599453,\n"," 'largest': 0.6931471805599453}"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Compute the term TF/IDF for all words"],"metadata":{"id":"0VCE4hEh-0yr"}},{"cell_type":"code","source":["def computeTFIDF(tfBagOfWords, idfs):\n","  tfidf = {}\n","  for word, val in tfBagOfWords.items():\n","    tfidf[word] = val*idfs[word]\n","  return tfidf\n","\n","tfidfA = computeTFIDF(tfA, idfs)\n","tfidfB = computeTFIDF(tfB, idfs)\n","df=pd.DataFrame([tfidfA, tfidfB])\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"E0r_A_bWQJAy","outputId":"87669c40-d292-4292-dcf1-10cb60033fec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   planet   Jupiter   is      from      Mars       Sun  the    fourth  \\\n","0     0.0  0.138629  0.0  0.000000  0.000000  0.000000  0.0  0.000000   \n","1     0.0  0.000000  0.0  0.086643  0.086643  0.086643  0.0  0.086643   \n","\n","    largest  \n","0  0.138629  \n","1  0.000000  "],"text/html":["\n","  <div id=\"df-d4289bb4-02ca-42aa-8470-03f0f8ab4168\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>planet</th>\n","      <th>Jupiter</th>\n","      <th>is</th>\n","      <th>from</th>\n","      <th>Mars</th>\n","      <th>Sun</th>\n","      <th>the</th>\n","      <th>fourth</th>\n","      <th>largest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.138629</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.138629</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.086643</td>\n","      <td>0.086643</td>\n","      <td>0.086643</td>\n","      <td>0.0</td>\n","      <td>0.086643</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4289bb4-02ca-42aa-8470-03f0f8ab4168')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d4289bb4-02ca-42aa-8470-03f0f8ab4168 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d4289bb4-02ca-42aa-8470-03f0f8ab4168');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":22}]}]}